{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ansu0122/ucu-mmds/blob/main/src/homework1/ucu_mmds_homework1_shared.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook has to be run in Docker jupyter/pyspark-notebook contained as Colab terminates the session before the fitting/evaluation completes on a full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vz4Gg28oUcSr",
    "outputId": "9ec930c3-438a-499e-d19f-2b7fd3fbf063"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: joblibspark in /opt/conda/lib/python3.11/site-packages (0.5.3)\n",
      "Requirement already satisfied: joblib>=0.14 in /opt/conda/lib/python3.11/site-packages (from joblibspark) (1.3.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install -q findspark\n",
    "!pip install joblibspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jf2XBqxtUmTM"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "version = \"3.5.0\"\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-17-openjdk-arm64\"\n",
    "os.environ[\"SPARK_HOME\"] = f\"/usr/local/spark\"\n",
    "!export PATH=$SPARK_HOME/bin:$PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gus5Y6IXWmFp"
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bfy8BLe5Wpyr"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import requests\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col, avg, when\n",
    "from pyspark.sql.functions import regexp_replace, col, lit, avg\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, StopWordsRemover, Normalizer, Word2Vec\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "from joblibspark import register_spark\n",
    "\n",
    "from pyspark.ml.feature import BucketedRandomProjectionLSH\n",
    "from pyspark.ml.evaluation import Evaluator\n",
    "from pyspark.ml import Estimator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from sklearn.utils import parallel_backend\n",
    "from pyspark.ml import PipelineModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark \n",
    "sc = pyspark.SparkContext('local[*]')\n",
    "# do something to prove it works\n",
    "rdd = sc.parallelize(range(1000))\n",
    "rdd.takeSample(False, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 219
    },
    "id": "qQgIL5nwY64d",
    "outputId": "6926266d-4ec9-4b2c-a628-8f5cb518599b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://c715a83e2c6a:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0xffff5ccb1550>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession(sc)\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r8UXkhdevhwy"
   },
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work\n"
     ]
    }
   ],
   "source": [
    "cd /home/jovyan/work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CKSpJ8gwqTVq",
    "outputId": "e11ce34a-b5a8-44ad-978b-33f8d2f188e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-11-08 21:08:49--  https://data.insideairbnb.com/spain/catalonia/barcelona/2024-09-06/data/listings.csv.gz\n",
      "Resolving data.insideairbnb.com (data.insideairbnb.com)... 108.138.51.6, 108.138.51.28, 108.138.51.93, ...\n",
      "Connecting to data.insideairbnb.com (data.insideairbnb.com)|108.138.51.6|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 9977937 (9.5M) [application/x-gzip]\n",
      "Saving to: ‘listings.csv.gz’\n",
      "\n",
      "listings.csv.gz     100%[===================>]   9.52M  1.11MB/s    in 9.0s    \n",
      "\n",
      "2024-11-08 21:08:58 (1.06 MB/s) - ‘listings.csv.gz’ saved [9977937/9977937]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -O listings.csv.gz \"https://data.insideairbnb.com/spain/catalonia/barcelona/2024-09-06/data/listings.csv.gz\"\n",
    "!gunzip -f listings.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2P_WOQ2BSHQp",
    "outputId": "b94f878f-0803-4876-8834-5ccade25838c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------+------------+-----------+--------------------+--------------------+---------------------+--------------------+-------+--------------------+--------------+----------+----------------+--------------------+------------------+------------------+--------------------+-----------------+--------------------+--------------------+--------------------+-------------------+-------------------------+--------------------+--------------------+----------------------+--------------------+----------------------+----------------------------+-----------------+-----------------+------------------+---------------+------------+---------+--------------+--------+----+--------------------+-------+--------------+--------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------+----------------+---------------+---------------+---------------+----------------+---------------------+-----------------+---------------------+----------------------+------------+-----------+--------------------+----------------------+-------------------------+---------------------+---------------------------+----------------------+-------------------+-----------+----------------+------------------------------+-------------------------------------------+--------------------------------------------+-------------------------------------------+-----------------+\n",
      "|   id|         listing_url|     scrape_id|last_scraped|     source|                name|         description|neighborhood_overview|         picture_url|host_id|            host_url|     host_name|host_since|   host_location|          host_about|host_response_time|host_response_rate|host_acceptance_rate|host_is_superhost|  host_thumbnail_url|    host_picture_url|  host_neighbourhood|host_listings_count|host_total_listings_count|  host_verifications|host_has_profile_pic|host_identity_verified|       neighbourhood|neighbourhood_cleansed|neighbourhood_group_cleansed|         latitude|        longitude|     property_type|      room_type|accommodates|bathrooms|bathrooms_text|bedrooms|beds|           amenities|  price|minimum_nights|maximum_nights|minimum_minimum_nights|maximum_minimum_nights|minimum_maximum_nights|maximum_maximum_nights|minimum_nights_avg_ntm|maximum_nights_avg_ntm|calendar_updated|has_availability|availability_30|availability_60|availability_90|availability_365|calendar_last_scraped|number_of_reviews|number_of_reviews_ltm|number_of_reviews_l30d|first_review|last_review|review_scores_rating|review_scores_accuracy|review_scores_cleanliness|review_scores_checkin|review_scores_communication|review_scores_location|review_scores_value|    license|instant_bookable|calculated_host_listings_count|calculated_host_listings_count_entire_homes|calculated_host_listings_count_private_rooms|calculated_host_listings_count_shared_rooms|reviews_per_month|\n",
      "+-----+--------------------+--------------+------------+-----------+--------------------+--------------------+---------------------+--------------------+-------+--------------------+--------------+----------+----------------+--------------------+------------------+------------------+--------------------+-----------------+--------------------+--------------------+--------------------+-------------------+-------------------------+--------------------+--------------------+----------------------+--------------------+----------------------+----------------------------+-----------------+-----------------+------------------+---------------+------------+---------+--------------+--------+----+--------------------+-------+--------------+--------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------+----------------+---------------+---------------+---------------+----------------+---------------------+-----------------+---------------------+----------------------+------------+-----------+--------------------+----------------------+-------------------------+---------------------+---------------------------+----------------------+-------------------+-----------+----------------+------------------------------+-------------------------------------------+--------------------------------------------+-------------------------------------------+-----------------+\n",
      "|18674|https://www.airbn...|20240906140800|  2024-09-06|city scrape|Huge flat for 8 p...|110m2 apartment t...| Apartment in Barc...|https://a0.muscac...|  71615|https://www.airbn...| Mireia  Maria|2010-01-19|Barcelona, Spain|We are Mireia (47...|    within an hour|               97%|                 88%|                f|https://a0.muscac...|https://a0.muscac...|  la Sagrada Família|                 43|                       48|  ['email', 'phone']|                   t|                     t|Barcelona, CT, Spain|    la Sagrada Família|                    Eixample|         41.40556|          2.17262|Entire rental unit|Entire home/apt|           8|      2.0|       2 baths|       3|   6|[\"Free street par...|$194.00|             1|          1125|                     1|                     4|                  1125|                  1125|                   1.5|                1125.0|            NULL|               t|              6|             19|             42|              72|           2024-09-06|               42|                    3|                     0|  2013-05-27| 2024-06-03|                4.37|                  4.46|                     4.61|                 4.76|                       4.68|                  4.78|               4.32|HUTB-002062|               t|                            28|                                         28|                                           0|                                          0|             0.31|\n",
      "|23197|https://www.airbn...|20240906140800|  2024-09-06|city scrape|Forum CCIB DeLuxe...|Beautiful spaciou...| Strategically loc...|https://a0.muscac...|  90417|https://www.airbn...|Etain (Marnie)|2010-03-09|Catalonia, Spain|Hi there, \\nI'm m...|    within an hour|              100%|                 96%|                t|https://a0.muscac...|https://a0.muscac...|El Besòs i el Mar...|                  6|                       13|  ['email', 'phone']|                   t|                     t|Sant Adria de Bes...|  el Besòs i el Mar...|                  Sant Martí|41.41243172529066|2.219750335269476|Entire rental unit|Entire home/apt|           5|      2.0|       2 baths|       3|   4|[\"Free street par...|$304.00|             3|            32|                     2|                     5|                  1125|                  1125|                   3.4|                1125.0|            NULL|               t|             10|             28|             58|             116|           2024-09-06|               79|                    7|                     0|  2011-03-15| 2024-06-24|                4.79|                  4.94|                      4.9|                 4.94|                       4.99|                  4.62|               4.64| HUTB005057|               f|                             1|                                          1|                                           0|                                          0|             0.48|\n",
      "|32711|https://www.airbn...|20240906140800|  2024-09-06|city scrape|Sagrada Familia a...|A lovely two bedr...| What's nearby  <b...|https://a0.muscac...| 135703|https://www.airbn...|          Nick|2010-05-31|Barcelona, Spain|I'm Nick your Eng...|    within an hour|              100%|                 99%|                f|https://a0.muscac...|https://a0.muscac...|Camp d'en Grassot...|                  3|                       15|['email', 'phone'...|                   t|                     t|Barcelona, Catalo...|  el Camp d'en Gras...|                      Gràcia|         41.40566|          2.17015|Entire rental unit|Entire home/apt|           6|      1.5|     1.5 baths|       2|   3|[\"Microwave\", \"Cr...|$211.00|             1|            31|                     1|                     5|                    31|                    31|                   1.1|                  31.0|            NULL|               t|              8|             20|             48|             319|           2024-09-06|              128|                   32|                     4|  2011-07-17| 2024-09-05|                4.46|                  4.45|                      4.4|                 4.89|                       4.88|                  4.88|               4.49|HUTB-001722|               f|                             3|                                          3|                                           0|                                          0|              0.8|\n",
      "|34241|https://www.airbn...|20240906140800|  2024-09-06|city scrape|Stylish Top Floor...|Located in close ...|                 NULL|https://a0.muscac...|  73163|https://www.airbn...|        Andres|2010-01-24|Barcelona, Spain|Hello I am a Prof...|    within an hour|              100%|                100%|                f|https://a0.muscac...|https://a0.muscac...|            El Gòtic|                  4|                        4|['email', 'phone'...|                   t|                     t|                NULL|        el Barri Gòtic|                Ciutat Vella|         41.38062|          2.17517|      Entire condo|Entire home/apt|           2|      1.0|        1 bath|       1|   1|[\"Microwave\", \"Ov...|$300.00|             5|           120|                     4|                     5|                   120|                   120|                   5.0|                 120.0|            NULL|               t|             16|             39|             69|              69|           2024-09-06|               20|                   12|                     1|  2010-07-10| 2024-08-20|                4.47|                  4.59|                     4.59|                 4.59|                       4.82|                  4.76|               4.35|     Exempt|               f|                             3|                                          3|                                           0|                                          0|             0.12|\n",
      "|34981|https://www.airbn...|20240906140800|  2024-09-06|city scrape|VIDRE HOME PLAZA ...|Spacious apartmen...| Located in Ciutat...|https://a0.muscac...|  73163|https://www.airbn...|        Andres|2010-01-24|Barcelona, Spain|Hello I am a Prof...|    within an hour|              100%|                100%|                f|https://a0.muscac...|https://a0.muscac...|            El Gòtic|                  4|                        4|['email', 'phone'...|                   t|                     t|Barcelona, Catalo...|        el Barri Gòtic|                Ciutat Vella|         41.37978|          2.17623|Entire rental unit|Entire home/apt|           9|      3.0|       3 baths|       4|   6|[\"Microwave\", \"Ho...|$314.00|             5|           365|                     2|                     5|                  1125|                  1125|                   4.9|                1125.0|            NULL|               t|              9|             35|             65|             241|           2024-09-06|              247|                   37|                     3|  2010-10-03| 2024-09-02|                4.53|                  4.59|                     4.64|                 4.67|                        4.7|                  4.67|               4.42|HUTB-150671|               f|                             3|                                          3|                                           0|                                          0|             1.46|\n",
      "+-----+--------------------+--------------+------------+-----------+--------------------+--------------------+---------------------+--------------------+-------+--------------------+--------------+----------+----------------+--------------------+------------------+------------------+--------------------+-----------------+--------------------+--------------------+--------------------+-------------------+-------------------------+--------------------+--------------------+----------------------+--------------------+----------------------+----------------------------+-----------------+-----------------+------------------+---------------+------------+---------+--------------+--------+----+--------------------+-------+--------------+--------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------+----------------+---------------+---------------+---------------+----------------+---------------------+-----------------+---------------------+----------------------+------------+-----------+--------------------+----------------------+-------------------------+---------------------+---------------------------+----------------------+-------------------+-----------+----------------+------------------------------+-------------------------------------------+--------------------------------------------+-------------------------------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "listings = spark.read.csv('./listings.csv', header=True, inferSchema=True, multiLine=True, escape='\"')\n",
    "listings.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cR65EF5woN2V",
    "outputId": "d1043a43-b36b-4323-99bf-e1e1ed4b8cf7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                name|         description|\n",
      "+--------------------+--------------------+\n",
      "|Huge flat for 8 p...|110m2 apartment t...|\n",
      "|Forum CCIB DeLuxe...|Beautiful spaciou...|\n",
      "|Sagrada Familia a...|A lovely two bedr...|\n",
      "|Stylish Top Floor...|Located in close ...|\n",
      "|VIDRE HOME PLAZA ...|Spacious apartmen...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "listings_desc = listings.select('name','description').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wdWBPjxVvmTb"
   },
   "source": [
    "# Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VQ7he7g52BFI"
   },
   "outputs": [],
   "source": [
    "def run_tf_idf(input: DataFrame, input_col: str, features_num: int, stop_words: list) -> DataFrame:\n",
    "\n",
    "    tokenizer = Tokenizer(inputCol=input_col, outputCol=\"words\")\n",
    "    general_remover = StopWordsRemover(inputCol=\"words\", outputCol=\"cleanTokens\")\n",
    "    context_remover = StopWordsRemover(inputCol=\"cleanTokens\", outputCol=\"myCleanTokens\").setStopWords(stop_words)\n",
    "    hashingTF = HashingTF(inputCol=\"myCleanTokens\", outputCol=\"rawFeatures\", numFeatures=features_num)\n",
    "    idf = IDF(inputCol=\"rawFeatures\", outputCol=\"vectorSpace\")\n",
    "    normalizer = Normalizer(inputCol=\"vectorSpace\", outputCol=\"normVectorSpace\")\n",
    "\n",
    "    pipeline = Pipeline(stages=[tokenizer, general_remover, context_remover, hashingTF, idf, normalizer])\n",
    "    model = pipeline.fit(input)\n",
    "    results = model.transform(input)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GISv210F5Kp5"
   },
   "outputs": [],
   "source": [
    "def run_word2vec(input: DataFrame, input_col: str, stop_words: list, features_num: int, minCount: int=2) -> DataFrame:\n",
    "\n",
    "    tokenizer = Tokenizer(inputCol=input_col, outputCol=\"words\")\n",
    "    general_remover = StopWordsRemover(inputCol=\"words\", outputCol=\"cleanTokens\")\n",
    "    context_remover = StopWordsRemover(inputCol=\"cleanTokens\", outputCol=\"myCleanTokens\").setStopWords(stop_words)\n",
    "    word2Vec = Word2Vec(vectorSize=features_num, minCount=minCount, inputCol=\"myCleanTokens\", outputCol=\"vectorSpace\")\n",
    "    normalizer = Normalizer(inputCol=\"vectorSpace\", outputCol=\"normVectorSpace\")\n",
    "\n",
    "    pipeline = Pipeline(stages=[tokenizer, general_remover, context_remover, word2Vec, normalizer])\n",
    "    model = pipeline.fit(input)\n",
    "    results = model.transform(input)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J_Fg0ySQYsfw"
   },
   "outputs": [],
   "source": [
    "def compute_ground_truth(df: DataFrame, input_col: str, id_col: str, num_neighbors: int = 3):\n",
    "\n",
    "    sample_df = df.select(id_col, input_col).collect()\n",
    "    sample_features = np.array([row[input_col].toArray() if hasattr(row[input_col], 'toArray') else row[input_col]\n",
    "                                for row in sample_df])\n",
    "\n",
    "    ids = [row[id_col] for row in sample_df]\n",
    "    nn = NearestNeighbors(n_neighbors=num_neighbors + 1, metric='cosine')\n",
    "    nn.fit(sample_features)\n",
    "\n",
    "    neighbors_indices = nn.kneighbors(sample_features, return_distance=False)\n",
    "    ground_truth_ids = [[ids[idx] for idx in neighbors[1:]] for neighbors in neighbors_indices]\n",
    "\n",
    "    return ground_truth_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1iHBiMtIdXhF"
   },
   "outputs": [],
   "source": [
    "def calculate_precision(found_neighbors: list, ground_truth: list):\n",
    "    precision_scores = [\n",
    "            len(set(found) & set(truth)) / len(truth) for found, truth in zip(found_neighbors, ground_truth)\n",
    "        ]\n",
    "    return np.mean(precision_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "QJQepJFbtSMa"
   },
   "outputs": [],
   "source": [
    "class PrecisionEvaluator(Evaluator):\n",
    "    def __init__(self, input_col: str, output_col: str, id_col: str, ground_truth: list, num_neighbors: int = 5):\n",
    "        self.input_col = input_col\n",
    "        self.output_col = output_col\n",
    "        self.ground_truth = ground_truth\n",
    "        self.id_col = id_col\n",
    "        self.num_neighbors = num_neighbors\n",
    "\n",
    "    def _evaluate(self, dataset: DataFrame, model: BucketedRandomProjectionLSH):\n",
    "\n",
    "        dataset = dataset.withColumnRenamed(self.output_col, f\"{self.output_col}_temp\")\n",
    "\n",
    "        return evaluate_lsh_model(dataset, model, self.input_col, self.id_col, self.ground_truth, self.num_neighbors)\n",
    "\n",
    "    def isLargerBetter(self):\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "XxfAgtbkVHzZ"
   },
   "outputs": [],
   "source": [
    "def evaluate_lsh_model(\n",
    "                      dataset: DataFrame,\n",
    "                      model: BucketedRandomProjectionLSH,\n",
    "                      input_col: str,\n",
    "                      id_col: str,\n",
    "                      ground_truth: list,\n",
    "                      num_neighbors: int = 5\n",
    "                      ):\n",
    "    sample_features = dataset.select(id_col, input_col).collect()\n",
    "    found_neighbors = []\n",
    "\n",
    "    for feature_row in sample_features:\n",
    "        current_id = feature_row[id_col]\n",
    "        query_vector = feature_row[input_col]\n",
    "\n",
    "        # find nearest neighbors using approxNearestNeighbors\n",
    "        neighbors = model.approxNearestNeighbors(dataset, query_vector, num_neighbors+1)\n",
    "\n",
    "        # filter out the current record by `id_col`\n",
    "        neighbors_list = [row[id_col] for row in neighbors.collect() if row[id_col] != current_id]\n",
    "\n",
    "        found_neighbors.append(neighbors_list)\n",
    "\n",
    "    return calculate_precision(found_neighbors, ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "vPB1QnpCBc5C"
   },
   "outputs": [],
   "source": [
    "class CustomCrossValidator(CrossValidator):\n",
    "    def __init__(self,\n",
    "                 estimator: Estimator,\n",
    "                 estimatorParamMaps: list,\n",
    "                 evaluator: Evaluator,\n",
    "                 numFolds: int = 1):\n",
    "        super().__init__(estimator=estimator, estimatorParamMaps=estimatorParamMaps, evaluator=evaluator, numFolds=numFolds)\n",
    "\n",
    "    def _fit(self, dataset: DataFrame):\n",
    "\n",
    "        best_model = None\n",
    "        best_score = float('-inf')\n",
    "        best_params = None\n",
    "\n",
    "        v_numFolds = self.getOrDefault(self.numFolds)\n",
    "        if isinstance(v_numFolds, int) and v_numFolds > 0:\n",
    "            # split dataset into folds\n",
    "            splits = dataset.randomSplit([1.0 / v_numFolds] * v_numFolds)\n",
    "        else:\n",
    "            raise ValueError(f\"Number of folds should be a positive integer: {v_numFolds}\")\n",
    "\n",
    "        split_data = []\n",
    "        for i in range(v_numFolds):\n",
    "            test_data = splits[i]\n",
    "            train_data = [splits[j] for j in range(v_numFolds) if j != i]\n",
    "            train_data = train_data[0] if len(train_data) == 1 else train_data[0].union(*train_data[1:])\n",
    "            split_data.append((train_data, test_data))\n",
    "\n",
    "        for ip, param_map in enumerate(self.getOrDefault(self.estimatorParamMaps)):\n",
    "            fold_metrics = []\n",
    "            for il, (train_data, test_data) in enumerate(split_data):\n",
    "                estimator_with_params = self.getOrDefault(self.estimator).copy(param_map)\n",
    "                model = estimator_with_params.fit(train_data)\n",
    "                print(f'train/eval loop index {il}, hash tab number {param_map.get(model.numHashTables)}')\n",
    "                # passing model to the evaluator\n",
    "                metric = self.getOrDefault(self.evaluator)._evaluate(test_data, model)\n",
    "                fold_metrics.append(metric)\n",
    "\n",
    "            avg_metric = np.mean(fold_metrics)\n",
    "\n",
    "            if avg_metric > best_score:\n",
    "                best_score = avg_metric\n",
    "                best_model = model\n",
    "                best_params = param_map\n",
    "\n",
    "        return best_model, best_params, best_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "id": "K9agsZ6kkUzP"
   },
   "outputs": [],
   "source": [
    "class CustomValidator(CrossValidator):\n",
    "    def __init__(self,\n",
    "                 estimator: Estimator,\n",
    "                 estimatorParamMaps: list,\n",
    "                 evaluator: Evaluator,\n",
    "                 numFolds: int = 1):\n",
    "        super().__init__(estimator=estimator, estimatorParamMaps=estimatorParamMaps, evaluator=evaluator, numFolds=numFolds)\n",
    "\n",
    "    def _fit(self, dataset: DataFrame):\n",
    "\n",
    "        best_model = None\n",
    "        best_score = float('-inf')\n",
    "        best_params = None\n",
    "\n",
    "        for ip, param_map in enumerate(self.getOrDefault(self.estimatorParamMaps)):\n",
    "            train_data, test_data = dataset.randomSplit([0.8, 0.2])\n",
    "            estimator_with_params = self.getOrDefault(self.estimator).copy(param_map)\n",
    "            model = estimator_with_params.fit(train_data)\n",
    "            print(f'hash tab number -> {param_map.get(model.numHashTables)}')\n",
    "            print(f'bucket length -> {param_map.get(model.bucketLength)}')\n",
    "            # passing model to the evaluator\n",
    "            avg_metric = self.getOrDefault(self.evaluator)._evaluate(test_data, model)\n",
    "\n",
    "            if avg_metric > best_score:\n",
    "                best_score = avg_metric\n",
    "                best_model = model\n",
    "                best_params = param_map\n",
    "\n",
    "        return best_model, best_params, best_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OSiskcxEvAqh"
   },
   "source": [
    "# Airbnb properties in Barcelona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "S5mVEf-gu1-b"
   },
   "outputs": [],
   "source": [
    "num_feat = 2**8\n",
    "num_neighbors = 5\n",
    "target_col = 'name'\n",
    "vector_space_col = 'normVectorSpace'\n",
    "output_col = 'hashes'\n",
    "id_col = 'id'\n",
    "\n",
    "\n",
    "rnb_stopwords = [\n",
    "    \"apartment\", \"flat\", \"property\", \"located\", \"near\", \"includes\", \"features\",\n",
    "    \"offered\", \"offering\", \"available\", \"spacious\", \"beautiful\", \"lovely\",\n",
    "    \"modern\", \"new\", \"floor\", \"building\", \"block\", \"complex\", \"close\", \"distance\",\n",
    "    \"minutes\", \"city\", \"center\", \"town\", \"square\", \"station\", \"transport\", \"shops\",\n",
    "    \"amenities\", \"service\", \"services\", \"access\", \"perfect\", \"ideal\", \"suitable\",\n",
    "    \"designed\", \"high-quality\", \"quality\", \"luxury\", \"premium\", \"affordable\", \"cheap\",\n",
    "    \"price\", \"cost\", \"included\", \"terms\", \"conditions\", \"contact\", \"call\", \"schedule\",\n",
    "    \"view\", \"appointment\", \"now\", \"immediate\", \"moving\", \"move-in\", \"move\", \"ready\",\n",
    "    \"relax\", \"cozy\", \"charming\", \"comfortable\", \"beautiful\", \"peaceful\", \"stylish\",\n",
    "    \"great\", \"amazing\", \"lovely\", \"nice\", \"serene\", \"elegant\", \"bright\", \"spacious\",\n",
    "    \"private\", \"unique\", \"friendly\", \"warm\", \"luxurious\", \"vibrant\", \"inviting\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F6q5JNczlokc",
    "outputId": "bda7c2ad-4888-450f-9203-960cc078114b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|summary|                  id|\n",
      "+-------+--------------------+\n",
      "|  count|               19482|\n",
      "|   mean|4.645670724989749...|\n",
      "| stddev|4.961103420827175E17|\n",
      "|    min|               18674|\n",
      "|    25%|            25402379|\n",
      "|    50%|            53390353|\n",
      "|    75%|  965071766227471616|\n",
      "|    max| 1239618639352958825|\n",
      "+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "listings.select('id').summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TWi7GAUSvSjr"
   },
   "source": [
    "### TF-IDF & LSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mma9aqyxvv0V"
   },
   "outputs": [],
   "source": [
    "tf_idf_results = run_tf_idf(listings, target_col, num_feat, rnb_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yl4qPMqEu8jU",
    "outputId": "a4349524-30f7-4361-bce3-e5caa103be60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15650\n",
      "3832\n"
     ]
    }
   ],
   "source": [
    "train_data_tfidf, test_data_tfidf = tf_idf_results.randomSplit([0.8, 0.2], seed=42)\n",
    "train_y_tfidf = compute_ground_truth(train_data_tfidf, vector_space_col, 'id', num_neighbors=num_neighbors)\n",
    "test_y_tfidf = compute_ground_truth(test_data_tfidf, vector_space_col, 'id', num_neighbors=num_neighbors)\n",
    "print(train_data_tfidf.count())\n",
    "print(test_data_tfidf.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KNDVZBrHtext",
    "outputId": "ec25e013-3d43-4062-8af6-5f210138a599"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/joblibspark/backend.py:110: UserWarning: Spark local mode doesn't support stage-level scheduling.\n",
      "  warnings.warn(\"Spark local mode doesn't support stage-level scheduling.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hash tab number -> 200\n",
      "hash tab number -> 500\n",
      "hash tab number -> 1000\n",
      "hash tab number -> 1500\n",
      "best score => 0.0007559055118110235\n",
      "Best Parameters:\n",
      "numHashTables: 1500\n",
      "outputCol: hashes\n",
      "bucketLength: 1.0\n",
      "inputCol: normVectorSpace\n",
      "CPU times: user 7min 10s, sys: 11.5 s, total: 7min 21s\n",
      "Wall time: 9h 2min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "lsh_model = BucketedRandomProjectionLSH(inputCol=vector_space_col, outputCol=output_col)\n",
    "\n",
    "# Define parameter grid with bucketLength and numHashTables\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(lsh_model.numHashTables, [200, 500, 1000, 1500]) \\\n",
    "    .addGrid(lsh_model.bucketLength, [1.0]) \\\n",
    "    .build()\n",
    "\n",
    "evaluator = PrecisionEvaluator(input_col=vector_space_col, output_col=output_col, id_col=id_col, ground_truth=train_y_tfidf, num_neighbors=num_neighbors)\n",
    "\n",
    "register_spark()\n",
    "\n",
    "with parallel_backend('spark', n_jobs=3):\n",
    "  crossval = CustomValidator(\n",
    "      estimator=lsh_model,\n",
    "      estimatorParamMaps=param_grid,\n",
    "      evaluator=evaluator,\n",
    "      numFolds=3\n",
    "  )\n",
    "\n",
    "best_model_tfidf, best_params, best_score = crossval.fit(train_data_tfidf)\n",
    "best_params = {param.name: value for param, value in best_model_tfidf.extractParamMap().items()}\n",
    "print(f\"best score => {best_score}\")\n",
    "print(\"Best Parameters:\")\n",
    "for param_name, param_value in best_params.items():\n",
    "    print(f\"{param_name}: {param_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the best model\n",
    "model_save_path = f'/home/jovyan/work/ftidf-model'\n",
    "pipeline_model = PipelineModel(stages=[best_model_tfidf])\n",
    "pipeline_model.write().overwrite().save(model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-1oTRQeVxokc"
   },
   "outputs": [],
   "source": [
    "# evaluate the best model\n",
    "with parallel_backend('spark', n_jobs=5):\n",
    "    best_model_tfidf = PipelineModel.load('/home/jovyan/work/tfidf-model').stages[-1]\n",
    "\n",
    "tfidf_precision = evaluate_lsh_model (\n",
    "    dataset = test_data_tfidf,\n",
    "    model = best_model_tfidf,\n",
    "    input_col = vector_space_col,\n",
    "    id_col = id_col,\n",
    "    ground_truth = test_y_tfidf,\n",
    "    num_neighbors = num_neighbors\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test precision 78%\n"
     ]
    }
   ],
   "source": [
    "print(f'test precision {tfidf_precision*100:.0f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4l34gFa4v8Bu"
   },
   "source": [
    "### W2V & LSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w2lMXVFUcsEa"
   },
   "outputs": [],
   "source": [
    "w2v_results = run_word2vec(listings, target_col, rnb_stopwords, num_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RXIxFpYJv7nP",
    "outputId": "080d96bf-bc26-4749-e084-1a12934df1eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3977\n",
      "15505\n"
     ]
    }
   ],
   "source": [
    "train_data_w2v, test_data_w2v = w2v_results.randomSplit([0.2, 0.8], seed=42)\n",
    "train_y_w2v = compute_ground_truth(train_data_w2v, vector_space_col, id_col, num_neighbors=num_neighbors)\n",
    "test_y_w2v = compute_ground_truth(test_data_w2v, vector_space_col, id_col, num_neighbors=num_neighbors)\n",
    "print(train_data_w2v.count())\n",
    "print(test_data_w2v.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NO71mleHdP4G",
    "outputId": "27bd332f-a5a1-462f-a3dd-dfc78e06b0b0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/joblibspark/backend.py:110: UserWarning: Spark local mode doesn't support stage-level scheduling.\n",
      "  warnings.warn(\"Spark local mode doesn't support stage-level scheduling.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hash tab number -> 50\n",
      "bucket length -> 0.5\n",
      "hash tab number -> 50\n",
      "bucket length -> 1.0\n",
      "hash tab number -> 50\n",
      "bucket length -> 2.0\n",
      "hash tab number -> 100\n",
      "bucket length -> 0.5\n",
      "hash tab number -> 100\n",
      "bucket length -> 1.0\n",
      "hash tab number -> 100\n",
      "bucket length -> 2.0\n",
      "hash tab number -> 200\n",
      "bucket length -> 0.5\n",
      "hash tab number -> 200\n",
      "bucket length -> 1.0\n",
      "hash tab number -> 200\n",
      "bucket length -> 2.0\n",
      "Best Parameters:\n",
      "numHashTables: 100\n",
      "outputCol: hashes\n",
      "bucketLength: 2.0\n",
      "inputCol: normVectorSpace\n",
      "CPU times: user 47.9 s, sys: 6.07 s, total: 54 s\n",
      "Wall time: 1h 42min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "lsh_model = BucketedRandomProjectionLSH(inputCol=vector_space_col, outputCol=output_col)\n",
    "\n",
    "# Define parameter grid with bucketLength and numHashTables\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(lsh_model.numHashTables, [50, 100, 200]) \\\n",
    "    .addGrid(lsh_model.bucketLength, [0.5, 1.0, 2.0]) \\\n",
    "    .build()\n",
    "\n",
    "evaluator = PrecisionEvaluator(input_col=vector_space_col, output_col=output_col, id_col=id_col, ground_truth=train_y_w2v, num_neighbors=num_neighbors)\n",
    "\n",
    "register_spark()\n",
    "\n",
    "with parallel_backend('spark', n_jobs=10):\n",
    "    crossval = CustomValidator(\n",
    "        estimator=lsh_model,\n",
    "        estimatorParamMaps=param_grid,\n",
    "        evaluator=evaluator,\n",
    "        numFolds=3\n",
    ")\n",
    "\n",
    "best_model_w2v, best_params, best_score = crossval.fit(train_data_w2v)\n",
    "best_params = {param.name: value for param, value in best_model_w2v.extractParamMap().items()}\n",
    "\n",
    "print(\"Best Parameters:\")\n",
    "for param_name, param_value in best_params.items():\n",
    "    print(f\"{param_name}: {param_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "model_save_path = f'/home/jovyan/work/w2v-model'\n",
    "pipeline_model = PipelineModel(stages=[best_model_w2v])\n",
    "pipeline_model.write().overwrite().save(model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qFiDUE3WfYfx"
   },
   "outputs": [],
   "source": [
    "# evaluate the best model\n",
    "\n",
    "with parallel_backend('spark', n_jobs=10):\n",
    "        best_model_w2v = PipelineModel.load('/home/jovyan/work/w2v-model').stages[-1]\n",
    "\n",
    "w2v_precision = evaluate_lsh_model (\n",
    "    dataset = test_data_w2v,\n",
    "    model = best_model_w2v,\n",
    "    input_col = vector_space_col,\n",
    "    id_col = id_col,\n",
    "    ground_truth = test_y_w2v,\n",
    "    num_neighbors = num_neighbors\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test precision 90%\n"
     ]
    }
   ],
   "source": [
    "print(f'test precision {w2v_precision*100:.0f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vLa7Os-62UHI"
   },
   "source": [
    "# Ukrainian Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "id": "tVz5H-6h-BD1"
   },
   "outputs": [],
   "source": [
    "url = \"https://wikimedia.org/api/rest_v1/metrics/pageviews/top/uk.wikisource/all-access/2019/04/all-days\"\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "    'Referer': 'https://www.example.com/'\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "if response.status_code != 200:\n",
    "    print(f\"Error: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DaCGkXY8-hub",
    "outputId": "cf4296ba-fde3-4d02-f05d-76f6e26bfb17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+-----+---+\n",
      "|             article|rank|views| id|\n",
      "+--------------------+----+-----+---+\n",
      "|    Головна сторінка|   1|21278|  0|\n",
      "|               Вірую|   2|14244|  1|\n",
      "|Мойсей Іван Франк...|   3| 2603|  2|\n",
      "|Закон України Про...|   4| 2576|  3|\n",
      "|Закон України Про...|   5| 1776|  4|\n",
      "| Конституція України|   6| 1684|  5|\n",
      "|   Архіви ДАЖО 178 3|   7| 1426|  6|\n",
      "|Конституція Пилип...|   8| 1248|  7|\n",
      "|     Конституція США|   9| 1221|  8|\n",
      "| Молитва за померлих|  10| 1212|  9|\n",
      "|Біблія Огієнко Но...|  11| 1100| 10|\n",
      "|Арфами арфами Тичина|  12| 1069| 11|\n",
      "|      Таємниця щастя|  13| 1047| 12|\n",
      "|            Отче наш|  14| 1011| 13|\n",
      "|Народні музичні і...|  15|  966| 14|\n",
      "|      Головна стаття|  16|  942| 15|\n",
      "|Природно заповідн...|  17|  924| 16|\n",
      "|Про Правила дорож...|  18|  918| 17|\n",
      "|Біблія Огієнко Но...|  19|  902| 18|\n",
      "|    Спеціальна Пошук|  20|  881| 19|\n",
      "+--------------------+----+-----+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    articles_list = data['items'][0]['articles']  # Adjust if accessing more nested structures\n",
    "\n",
    "    # Convert list of dictionaries to PySpark DataFrame\n",
    "    wiki_df = spark.createDataFrame(articles_list)\n",
    "    wiki_df = wiki_df.withColumn(\"id\", F.monotonically_increasing_id())\n",
    "\n",
    "    wiki_df = wiki_df.withColumn(\"article\", F.regexp_replace(\"article\", r\"[_\\-.,!?:;-@#$%^&*\\/(){}\\[\\]«»…»]\", \" \"))\n",
    "    wiki_df = wiki_df.withColumn(\"article\", F.regexp_replace(\"article\", r\"\\s+\", \" \"))\n",
    "    wiki_df = wiki_df.withColumn(\"article\", F.trim(\"article\"))\n",
    "\n",
    "    # Display the DataFrame\n",
    "    wiki_df.show()\n",
    "else:\n",
    "    print(f\"Error fetching data: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "opS-UCPE_UcZ",
    "outputId": "1029fc39-ce20-4bbb-e9cc-facd6a88ad39"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "975"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "id": "kMH6QGUL_qXW"
   },
   "outputs": [],
   "source": [
    "num_feat = 2**8\n",
    "num_neighbors = 5\n",
    "wiki_target_col = 'article'\n",
    "vector_space_col = 'normVectorSpace'\n",
    "output_col = 'hashes'\n",
    "id_col = 'id'\n",
    "\n",
    "\n",
    "wiki_stopwords = [\n",
    "    \"авжеж\", \"адже\", \"але\", \"б\", \"без\", \"був\", \"була\", \"були\", \"було\", \"бути\",\n",
    "    \"більш\", \"вам\", \"вас\", \"весь\", \"вздовж\", \"ви\", \"вниз\", \"внизу\", \"вона\", \"вони\",\n",
    "    \"воно\", \"все\", \"всередині\", \"всіх\", \"від\", \"він\", \"да\", \"давай\", \"давати\", \"де\",\n",
    "    \"дещо\", \"для\", \"до\", \"з\", \"завжди\", \"замість\", \"й\", \"коли\", \"ледве\", \"майже\", \"ми\",\n",
    "    \"навколо\", \"навіть\", \"нам\", \"от\", \"отже\", \"отож\", \"поза\", \"про\", \"під\", \"та\", \"так\",\n",
    "    \"такий\", \"також\", \"те\", \"ти\", \"тобто\", \"тож\", \"тощо\", \"хоча\", \"це\", \"цей\", \"чи\",\n",
    "    \"чого\", \"що\", \"як\", \"який\", \"якої\", \"є\", \"із\", \"інших\", \"їх\", \"її\",\n",
    "    \"в\", \"на\", \"до\", \"під\", \"з\", \"над\", \"по\", \"за\", \"від\", \"між\", \"перед\", \"через\",\n",
    "    \"без\", \"про\", \"для\", \"у\", \"внаслідок\", \"відносно\", \"завдяки\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "id": "r4Pai-jFA8kJ"
   },
   "outputs": [],
   "source": [
    "wiki_df = run_tf_idf(wiki_df, wiki_target_col, num_feat, wiki_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zcAh27fFBpP1",
    "outputId": "f557e4c3-6955-4491-fb8d-50ee6f69ad56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "793\n",
      "182\n"
     ]
    }
   ],
   "source": [
    "train_data_wiki, test_data_wiki = wiki_df.randomSplit([0.8, 0.2], seed=42)\n",
    "train_y_wiki = compute_ground_truth(train_data_wiki, vector_space_col, id_col, num_neighbors=num_neighbors)\n",
    "test_y_wiki = compute_ground_truth(test_data_wiki, vector_space_col, id_col, num_neighbors=num_neighbors)\n",
    "print(train_data_wiki.count())\n",
    "print(test_data_wiki.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HyUyDQZfIoyt",
    "outputId": "5934b3d8-0ca8-404d-b774-ecb7cf427d8f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/joblibspark/backend.py:110: UserWarning: Spark local mode doesn't support stage-level scheduling.\n",
      "  warnings.warn(\"Spark local mode doesn't support stage-level scheduling.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hash tab number -> 5\n",
      "bucket length -> 0.5\n",
      "hash tab number -> 5\n",
      "bucket length -> 1.0\n",
      "hash tab number -> 5\n",
      "bucket length -> 2.0\n",
      "hash tab number -> 10\n",
      "bucket length -> 0.5\n",
      "hash tab number -> 10\n",
      "bucket length -> 1.0\n",
      "hash tab number -> 10\n",
      "bucket length -> 2.0\n",
      "hash tab number -> 20\n",
      "bucket length -> 0.5\n",
      "hash tab number -> 20\n",
      "bucket length -> 1.0\n",
      "hash tab number -> 20\n",
      "bucket length -> 2.0\n",
      "hash tab number -> 50\n",
      "bucket length -> 0.5\n",
      "hash tab number -> 50\n",
      "bucket length -> 1.0\n",
      "hash tab number -> 50\n",
      "bucket length -> 2.0\n",
      "hash tab number -> 70\n",
      "bucket length -> 0.5\n",
      "hash tab number -> 70\n",
      "bucket length -> 1.0\n",
      "hash tab number -> 70\n",
      "bucket length -> 2.0\n",
      "Best Parameters:\n",
      "numHashTables: 70\n",
      "outputCol: hashes\n",
      "bucketLength: 1.0\n",
      "inputCol: normVectorSpace\n",
      "CPU times: user 9.44 s, sys: 2.31 s, total: 11.8 s\n",
      "Wall time: 6min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "lsh_model = BucketedRandomProjectionLSH(inputCol=vector_space_col, outputCol=output_col)\n",
    "\n",
    "# Define parameter grid with bucketLength and numHashTables\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(lsh_model.numHashTables, [5, 10, 20, 50, 70]) \\\n",
    "    .addGrid(lsh_model.bucketLength, [0.5, 1.0, 2.0]) \\\n",
    "    .build()\n",
    "\n",
    "evaluator = PrecisionEvaluator(\n",
    "    input_col=vector_space_col,\n",
    "    output_col=output_col,\n",
    "    id_col=id_col,\n",
    "    ground_truth=train_y_wiki,\n",
    "    num_neighbors=num_neighbors\n",
    ")\n",
    "\n",
    "register_spark()\n",
    "\n",
    "with parallel_backend('spark', n_jobs=5):\n",
    "    crossval = CustomValidator(\n",
    "        estimator=lsh_model,\n",
    "        estimatorParamMaps=param_grid,\n",
    "        evaluator=evaluator,\n",
    "        numFolds=3\n",
    "    )\n",
    "\n",
    "best_model_wiki, best_params, best_score = crossval.fit(train_data_wiki)\n",
    "best_params = {param.name: value for param, value in best_model_wiki.extractParamMap().items()}\n",
    "\n",
    "print(\"Best Parameters:\")\n",
    "for param_name, param_value in best_params.items():\n",
    "    print(f\"{param_name}: {param_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the best model\n",
    "model_save_path = f'/home/jovyan/work/wiki-model'\n",
    "pipeline_model = PipelineModel(stages=[best_model_wiki])\n",
    "pipeline_model.write().overwrite().save(model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "id": "2ffXDQT2KF8R"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/joblibspark/backend.py:110: UserWarning: Spark local mode doesn't support stage-level scheduling.\n",
      "  warnings.warn(\"Spark local mode doesn't support stage-level scheduling.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test precision 95.82%\n",
      "CPU times: user 1.18 s, sys: 225 ms, total: 1.4 s\n",
      "Wall time: 30.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# evaluate the best model\n",
    "\n",
    "with parallel_backend('spark', n_jobs=5):\n",
    "    best_model_wiki = PipelineModel.load('/home/jovyan/work/wiki-model').stages[-1]\n",
    "\n",
    "wiki_precision = evaluate_lsh_model(\n",
    "                                    dataset = test_data_wiki,\n",
    "                                    model = best_model_wiki,\n",
    "                                    input_col = vector_space_col,\n",
    "                                    id_col = id_col,\n",
    "                                    ground_truth = test_y_wiki,\n",
    "                                    num_neighbors = num_neighbors\n",
    "                                    )\n",
    "\n",
    "print(f'test precision {wiki_precision*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF/IDF/LSH model with WIKI data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "id": "Yhb-SWT5agYX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test precision 95.71%\n",
      "CPU times: user 9.87 s, sys: 190 ms, total: 10.1 s\n",
      "Wall time: 49.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "wiki_precision2 = evaluate_lsh_model(\n",
    "                                      dataset = test_data_wiki,\n",
    "                                      model = best_model_tfidf,\n",
    "                                      input_col = vector_space_col,\n",
    "                                      id_col = id_col,\n",
    "                                      ground_truth = test_y_wiki,\n",
    "                                      num_neighbors = num_neighbors\n",
    "                                      )\n",
    "print(f'test precision {wiki_precision2*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The accuracy of your model with at least 4 combinations of parameters\n",
    "\n",
    "#### We conducted three experiments:\n",
    "#### Experiment 1:\n",
    "- Model Type: LSH (BucketedRandomProjectionLSH)\n",
    "- Vectorization: TF/IDF (256)\n",
    "- Param Grid:\n",
    "    - lsh_model.numHashTables, [200, 500, 1000, 1500]\n",
    "    - lsh_model.bucketLength, [1.0]\n",
    "- Best Params: \n",
    "    - lsh_model.numHashTables = 1500, \n",
    "    - lsh_model.bucketLength = 1\n",
    "- Dataset: Airbnb Barcelona listings\n",
    "- Train/Test split: 80%/20% (15650/3832)\n",
    "- Test precision: 78%\n",
    "\n",
    "#### Experiment 2:\n",
    "- Model Type: LSH (BucketedRandomProjectionLSH)\n",
    "- Vectorization: Word2Vec (256)\n",
    "- Param Grid:\n",
    "    - lsh_model.numHashTables, [50, 100, 200]\n",
    "    - lsh_model.bucketLength, [0.5, 1.0, 2.0]\n",
    "- Best Params: \n",
    "    - lsh_model.numHashTables = 100, \n",
    "    - lsh_model.bucketLength = 2\n",
    "- Dataset: Airbnb Barcelona listings\n",
    "- Train/Test split: 20%/80% (3832/15650)\n",
    "- Test precision: 90%\n",
    "\n",
    "#### Experiment 3:\n",
    "- Model Type: LSH (BucketedRandomProjectionLSH)\n",
    "- Vectorization: TF/IDF (256)\n",
    "- Param Grid:\n",
    "    - lsh_model.numHashTables, [5, 10, 20, 50, 70]\n",
    "    - lsh_model.bucketLength, [0.5, 1.0, 2.0]\n",
    "- Best Params: \n",
    "    - lsh_model.numHashTables = 70, \n",
    "    - lsh_model.bucketLength = 1\n",
    "- Dataset: Ukrainian Wikipedia April 2019\n",
    "- Train/Test split: 80%/20% (793/182)\n",
    "- Test precision: 95.82%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The computation time spent in the parameter tuning procedure (specify also the characteristics of the machine(s) that you have used)\n",
    "#### Experiment 1:\n",
    "- Machine: Mac Pro M1 16 RAM\n",
    "- Parallel jobs: 3\n",
    "- Fine-tunning time: Wall time: 9h 2min 18s\n",
    "- Evaluation time: 5h 0min 2s\n",
    "\n",
    "#### Experiment 2:\n",
    "- Machine: Mac Pro M1 16 RAM\n",
    "- Parallel jobs: 10\n",
    "- Fine-tunning time: Wall time: 1h 42min 30s\n",
    "- Evaluation time: 11h 2min 28s\n",
    "\n",
    "#### Experiment 3:\n",
    "- Machine: Mac Pro M1 16 RAM\n",
    "- Parallel jobs: 5\n",
    "- Fine-tunning time: Wall time: 6min 12s\n",
    "- Evaluation time: Wall time: 30.9 s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What happen if we tune the parameters using Airbnb and the run with those parameters in The Wikipedia Dataset? What is the difference in accuracy with the parameters tuned directly in Wikipedia data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model Type: LSH (BucketedRandomProjectionLSH) fine-tuned on Airbnb data\n",
    "- Dataset: Ukrainian Wikipedia April 2019\n",
    "- Test precision 95.71%\n",
    "- Evaluation time: Wall time: 49.3 s\n",
    "\n",
    "Despite the fact that both datasets have different contextual meanings, we are achieving surprisingly good performance when evaluating the model on the Wiki dataset using an LSH model fine-tuned on Airbnb data. This is likely because the optimal number of hash tables for Airbnb is much larger than for the Wiki data, resulting in a certain degree of overfitting. However, it is evident that as the number of hash tables increases, the evaluation time also increases, which could be problematic in real-world scenarios."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
